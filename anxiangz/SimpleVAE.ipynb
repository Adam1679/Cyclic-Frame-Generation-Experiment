{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "hidden = 256       # size of the latent vector\n",
    "beta = 20          # hyperparameter for beta-VAE model\n",
    "M_N = 1            # hyperparameter for beta-VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import random\n",
    "from random import choices\n",
    "random.seed(52)\n",
    "data_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "your_name_dataset = ImageFolder(root=\"../png/142p_data\",\n",
    "                                transform=data_transform)\n",
    "N = len(your_name_dataset)\n",
    "sample_N = 6666\n",
    "sample_indices = choices(list(range(1, N)), k=sample_N)\n",
    "data_loader = torch.utils.data.DataLoader(your_name_dataset,\n",
    "                                          shuffle=False,\n",
    "                                          batch_size=batch_size,\n",
    "                                          pin_memory=True,\n",
    "                                          sampler=SubsetRandomSampler(sample_indices)\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = your_name_dataset[0][0].reshape(-1, 1).size(0)\n",
    "print(your_name_dataset[0][0].shape)\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def _cal_dim(x, k, s, p=0):\n",
    "    return (x - k + p) // s + 1\n",
    "    \n",
    "class View(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(self.dim)\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, latent=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.latent = latent\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        a, b = 2, 5\n",
    "        # Adjust the kernel size based on our input size 142 x 189. In the paper, the input is 64 x 64.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=(8, 9), stride=2),    # (B, -1, 142, 189)     \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 32, kernel_size=(6, 7), stride=2),             # (B, -1, 68, 91)     \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=(6, 7), stride=2),             # (B, -1, 32, 43)   \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 256, kernel_size=(6, 7), stride=2),            # (B, -1, 14, 19) \n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 512, kernel_size=(5, 7), stride=1),     # (B, -1, 5, 7)        \n",
    "            nn.ReLU(True),\n",
    "            View((-1, 512 * 1 * 1))          \n",
    "        )\n",
    "        self.fc_mu = nn.Linear(512 * 1 * 1, latent)\n",
    "        self.fc_sigma = nn.Linear(512 * 1 * 1, latent)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent, 512 * 1 * 1),             \n",
    "            View((-1, 512, 1, 1)),              \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=(5, 7), stride=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 64, kernel_size=(6, 7), stride=2), \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=(6, 7), stride=2), \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=(6, 7), stride=2), \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, in_channels, kernel_size=(8, 9), stride=2), \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_sigma(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.div(2).exp()\n",
    "        eps = Variable(torch.randn_like(std))\n",
    "        return eps * std + mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = _cal_dim(142, 8, 2)   # 68\n",
    "# x = _cal_dim(x, 6, 2)     # 32\n",
    "# x = _cal_dim(x, 6, 2)     # 14\n",
    "# x = _cal_dim(x, 6, 2)     #  5\n",
    "# x = _cal_dim(x, 5, 1)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = _cal_dim(189, 9, 2)  # 91\n",
    "# x = _cal_dim(x, 7, 2)     # 43\n",
    "# x = _cal_dim(x, 7, 2) # 19\n",
    "# x = _cal_dim(x, 7, 2) # 7\n",
    "# x = _cal_dim(x, 7, 1)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta, M_N):\n",
    "    b = x.size(0)\n",
    "    recon_x, x = recon_x.view(b, -1), x.view(b, -1)\n",
    "    recon_x = F.sigmoid(recon_x)\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Varational Bayes. ICLR,i 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + beta * M_N * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BetaVAE(latent=hidden)\n",
    "print(model)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "log_interval = 20\n",
    "length = len(data_loader) // batch_size\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta, M_N)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % length == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(data_loader),\n",
    "                100. * batch_idx / len(data_loader),\n",
    "                loss.item()))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss * batch_size / len(data_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
